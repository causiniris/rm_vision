# RoboTac 纯离线双轨视觉感知与预测系统 (rm_vision)

## 🌟 项目简介

本项目为基于 ROS 2 (Jazzy) 构建的 RoboTac/机甲大师视觉感知与多目标追踪 (MOT) 系统。
核心特色在于设计了**“纯离线零丢帧渲染架构”**与**“传统视觉 + 神经网络双轨独立预测模型”**。系统通过高度解耦的节点设计与超大容量的 QoS 通信队列，完美解决了离线算法调试中的掉帧错位问题，是压榨 2D 视觉追踪算法极限的绝佳工程实践。

## 🏗️ 系统架构设计

系统严格遵循 ROS 2 的分布式节点 (Node) 设计，主要包含以下核心模块：

1. **`Image_Source_Node` (图像源节点)**：负责以极高的稳定性发布本地测试视频流，并在 Launch 文件中被强制延迟启动，以等待底层通信管道建立。
2. **`Traditional_Vision_Node` (传统视觉节点)**：执行 OpenCV 预处理、灯条寻找与装甲板几何匹配。具有帧率高、但易受环境光干扰产生假阳性的特点。
3. **`Neural_Network_Node` (神经网络节点)**：加载 ONNX 格式的 YOLO 目标检测模型进行推理，鲁棒性极高，提供高置信度观测数据。
4. **`Predictor_Node` (预测与融合节点)**：系统的核心大脑。搭载了多目标追踪器 (MultiTracker)，利用卡尔曼滤波 (Kalman Filter) 进行状态估计，并内置 SORT 全局最优匹配与 `is_confirmed` 状态机。
5. **`Visualizer_Node` (可视化调试节点)**：纯离线大屏拼接器，将原图、传统视觉追踪画面、YOLO 追踪画面零丢帧拼接为“三联屏”并输出渲染视频。

## 🛠️ 核心算法演进与避坑总结

**阶段四：卡尔曼 MOT 开发与避坑总结**
本阶段成功构建了传统视觉与 YOLO 双轨独立调参的 2D 预测架构，利用智能指针与状态机解决了内存共享 Bug 和漏检丢帧问题。在挑战机器人“小陀螺”旋转时，我们尝试手写 EKF（扩展卡尔曼滤波）引入 CTRV 运动模型，却遭遇了严重的滤波器发散与 ID 爆炸。**失败根源**在于：平视视角的 2D 轨迹本质是简谐振动，强行套用 2D 圆周运动公式引发了灾难性的投影视差。最终果断回退至“线性 CV 模型 + 严格状态机”，通过重阻尼参数压榨出了 2D 追踪的极限。真正的非线性解算，必须留给未来的 3D PnP 升维架构。

* **分轨调参策略：**
  * **YOLO 轨道**：信任度高（较低 R 矩阵阻尼），长时记忆（保留 10 帧防掉帧），快速确认（2 帧即转正）。
  * **传统视觉轨道**：信任度极低（极大 R 矩阵阻尼），极短寿命（5 帧防残影），严格考核（4 帧确认防误检）。

## 🚀 编译与运行指南

### 1. 环境依赖
* ROS 2 Jazzy
* OpenCV 4.x
* cv_bridge
* 相应的 C++ 编译工具链

### 2. 编译项目
在工作空间根目录（`~/rm_vision`）下执行：
```bash
colcon build --packages-select rm_perception
```

### 3. 一键启动系统
项目已配备 vision_bringup.launch.py 一键启动脚本。该脚本会自动按正确时序拉起 5 个节点，并强制图像源节点延迟 3 秒启动，以确保 1000 深度容量的 QoS 队列完美建立。
```bash
# 刷新环境变量
source install/setup.bash

# 一键点火启动双轨感知系统
ros2 launch rm_perception vision_bringup.launch.py
```
运行结束后，可在工作空间根目录下查看渲染生成的 perception_demo_perfect.avi 三联屏实况录像。